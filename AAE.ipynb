{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Pytorch and Torchvision Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Torchgan Imports\n",
    "import torchgan.models as models\n",
    "import torchgan.losses as losses\n",
    "from torchgan.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "#set random seed\n",
    "seed = 999\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnist\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dsets.MNIST(\n",
    "    root = \"./mnist\",\n",
    "    train = True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5,),std=(0.5,)),\n",
    "        ]\n",
    "    ),\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = data.DataLoader(dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialAutoencoderGenerator(models.Generator):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoding_dims,\n",
    "            input_size,\n",
    "            input_channels,\n",
    "            step_channels=16,\n",
    "            nonlinearity=nn.LeakyReLU(0.2),\n",
    "    ):\n",
    "        super(AdversarialAutoencoderGenerator, self).__init__(encoding_dims)\n",
    "        encoder = [nn.Sequential(nn.Conv2d(input_channels, step_channels, 5, 2 ,2), nonlinearity\n",
    "                                 )\n",
    "                            ]\n",
    "        \n",
    "        size = input_size // 2\n",
    "        channels = step_channels\n",
    "        while size > 1:\n",
    "            encoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(channels, channels * 4, 5, 4, 2),\n",
    "                    nn.BatchNorm2d(channels * 4),\n",
    "                    nonlinearity,\n",
    "                )\n",
    "            )\n",
    "            channels *= 4\n",
    "            size = size // 4\n",
    "\n",
    "            self.encoder = nn.Sequential(*encoder)\n",
    "            self.encoder_fc = nn.Linear(\n",
    "                channels, encoding_dims\n",
    "            )\n",
    "            self.decoder_fc = nn.Linear(encoding_dims, step_channels)\n",
    "            decoder = []\n",
    "            size = 1\n",
    "            channels = step_channels\n",
    "            while size < input_size // 2:\n",
    "                decoder.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.ConvTranspose2d(channels, channels * 4, 5, 4, 2, 3),\n",
    "                        nn.BatchNorm2d(channels * 4),\n",
    "                        nonlinearity,\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            channels *= 4\n",
    "            size *= 4\n",
    "            decoder.append(nn.ConvTranspose2d(channels, input_channels, 5, 2, 2, 1))\n",
    "            self.decoder = nn.Sequential(*decoder)\n",
    "\n",
    "    def sample(self, noise):\n",
    "        noise = self.decoder_fc(noise)\n",
    "        noise = noise.view(-1, noise.size(1), 1, 1)\n",
    "        return self.decoder(noise)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            encoding = self.encoder(x)\n",
    "            encoding = self.encoder_fc(\n",
    "                encoding.view(\n",
    "                    -1, encoding.size(1) * encoding.size(2) * encoding.size(3)\n",
    "                )\n",
    "            )\n",
    "            return self.sample(encoding), encoding\n",
    "        else:\n",
    "            return self.sample(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disciminiator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialAutoencoderDiscriminator(models.Discriminator):\n",
    "    def __init__(self, input_dims, nonlinearity = nn.LeakyReLU(0.2)):\n",
    "        super(AdversarialAutoencoderDiscriminator, self).__init__(input_dims)\n",
    "        model = [nn.Sequential(nn.Linear(input_dims, input_dims // 2), nonlinearity)]\n",
    "        size = input_dims // 2\n",
    "        while size > 16:\n",
    "            model.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(size, size // 2), nn.BatchNorm1d(size // 2), nonlinearity\n",
    "                )\n",
    "            )\n",
    "        size = size // 2\n",
    "        model.append(nn.Linear(size, 1))\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions\n",
    "1. Reconstruction error: MSE between input signal and reconstruction\n",
    "2. Negative log likelihood of generated noise wrt discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class AdversarialAutoencoderGeneratorLoss(losses.GeneratorLoss):\n",
    "    def forward(self, real_inputs, gen_inputs, dgz):\n",
    "        loss = 0.999 * F.mse_loss(gen_inputs, real_inputs)\n",
    "        target = torch.ones_like(dgz)\n",
    "        loss += 0.001 * F.binary_cross_entropy_with_logits(dgz, target)\n",
    "        return loss\n",
    "\n",
    "    def train_ops(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        optimizer_generator,\n",
    "        real_inputs,\n",
    "        device,\n",
    "        batch_size,\n",
    "        labels=None,\n",
    "    ):\n",
    "        recon, encodings = generator(real_inputs)\n",
    "        optimizer_generator.zero_grad()\n",
    "        dgz = discriminator(encodings)\n",
    "        loss = self.forward(real_inputs, recon, dgz)\n",
    "        loss.backward()\n",
    "        optimizer_generator.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "class AdversarialAutoencoderDiscriminatorLoss(losses.DiscriminatorLoss):\n",
    "    def forward(self, dx, dgz):\n",
    "        target_real = torch.ones_like(dx)\n",
    "        target_fake = torch.zeros_like(dx)\n",
    "        loss = 0.5 * F.binary_cross_entropy_with_logits(dx, target_real)\n",
    "        loss += 0.5 * F.binary_cross_entropy_with_logits(dgz, target_fake)\n",
    "        return loss\n",
    "\n",
    "    def train_ops(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        optimizer_discriminator,\n",
    "        real_inputs,\n",
    "        device,\n",
    "        batch_size,\n",
    "        labels=None,\n",
    "    ):\n",
    "        _, encodings = generator(real_inputs)\n",
    "        noise = torch.randn(real_inputs.size(0), generator.encoding_dims, device=device)\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        dx = discriminator(noise)\n",
    "        dgz = discriminator(encodings)\n",
    "        loss = self.forward(dx, dgz)\n",
    "        loss.backward()\n",
    "        optimizer_discriminator.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "losses = [\n",
    "    AdversarialAutoencoderGeneratorLoss(),\n",
    "    AdversarialAutoencoderDiscriminatorLoss(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_weights: <HDF5 group \"/model_weights\" (21 members)>\n",
      "optimizer_weights: <HDF5 group \"/optimizer_weights\" (1 members)>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "filename = 'model_application.h5'\n",
    "h5 = h5py.File(filename,'r')\n",
    "for item in h5.keys():\n",
    "    print(item + \":\", h5[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d\n",
      "conv1d_1\n",
      "conv1d_2\n",
      "conv1d_3\n",
      "conv1d_4\n",
      "conv1d_5\n",
      "conv1d_6\n",
      "conv1d_transpose\n",
      "conv1d_transpose_1\n",
      "conv1d_transpose_2\n",
      "conv1d_transpose_3\n",
      "conv1d_transpose_4\n",
      "conv1d_transpose_5\n",
      "input_1\n",
      "max_pooling1d\n",
      "max_pooling1d_1\n",
      "max_pooling1d_2\n",
      "top_level_model_weights\n",
      "up_sampling1d\n",
      "up_sampling1d_1\n",
      "up_sampling1d_2\n"
     ]
    }
   ],
   "source": [
    "for item in h5.require_group('model_weights').keys():\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
